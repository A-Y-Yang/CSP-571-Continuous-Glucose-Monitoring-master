---
title: "CGM data LSTM"
output: html_notebook
author: Jawahar J. Panchal, 
        Illinois Institute of Technology
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
library(keras)
library(tensorflow)
#install_keras()
#install_tensorflow(version = "nightly")
```

# data preparation
```{r}
library(ggplot2)
head(economics)
```

# Rescale the data
`LSTM model requires to rescale the input data`. `Mean` and `standard deviation` of the training dataset can be used as the scaling coefficients to scale both the training and testing data sets as well as the predicted values. This way we ensure that the scaling does not impact the model.
```{r}
scale_factors <- c(mean(economics$unemploy), sd(economics$unemploy))
```

# Creating the prediction
If you wish to train-test the model, you should start with data split. 
As I focused on creating the prediction, not accuracy of the model itself, I used full dataset for training.
```{r}
scaled_train <- economics %>%
    dplyr::select(unemploy) %>%
    dplyr::mutate(unemploy = (unemploy - scale_factors[1]) / scale_factors[2])
```

Additionally `keras LSTM` expects specific tensor format of shape of a 3D array of the form `[samples, timesteps, features]` for `predictors (X)` and for `target (Y)` values:

-- `samples` specifies `the number of observations` which will be processed in batches.
-- `timesteps` tells us the number of time steps (lags). Or in other words `how many units back in time we want our network to see`.
-- `features` specifies `number of predictors (1 for univariate series` and n for multivariate)

In case of `predictors` that translates to an array of dimensions: (`nrow(data)` – `lag` – `prediction + 1`, `12`, `1`), where `lag = prediction = 12`.

```{r}
prediction <- 12
lag <- prediction
```

```{r}
scaled_train <- as.matrix(scaled_train)
 
# we lag the data 11 times and arrange that into columns
x_train_data <- t(sapply(
    1:(length(scaled_train) - lag - prediction + 1),
    function(x) scaled_train[x:(x + lag - 1), 1]
  ))
 
# now we transform it into 3D form
x_train_arr <- array(
    data = as.numeric(unlist(x_train_data)),
    dim = c(
        nrow(x_train_data),
        lag,
        1
    ))
```

Data was turned into a 3D array. As we have only one predictor, last dimension equals to one.

 
Now we apply similar transformation for the Y values.
```{r}
y_train_data <- t(sapply(
    (1 + lag):(length(scaled_train) - prediction + 1),
    function(x) scaled_train[x:(x + prediction - 1)]
))

y_train_arr <- array(
    data = as.numeric(unlist(y_train_data)),
    dim = c(
        nrow(y_train_data),
        prediction,
        1
    )
)
```

In the same manner we need to prepare input data for the prediction, which are `in fact last 12 observations from our training set`.
```{r}
x_test <- economics$unemploy[(nrow(scaled_train) - prediction + 1):nrow(scaled_train)]
```

We need to `scale and transform it`.
```{r}
# scale the data with same scaling factors as for training
x_test_scaled <- (x_test - scale_factors[1]) / scale_factors[2]

# this time our array just has one sample, as we intend to perform one 12-months prediction
x_pred_arr <- array(
    data = x_test_scaled,
    dim = c(
        1,
        lag,
        1
    )
)
```

# lstm prediction
We can `build a LSTM model` using the `keras_model_sequential` function and adding layers on top of that. 
The `first LSTM layer` takes the `required input shape`, which is the `[samples, timesteps, features]`.
We set for both layers return_sequences = TRUE and stateful = TRUE. 
The last layer is the same with the exception of batch_input_shape, which only needs to be specified in the first layer.
```{r}

lstm_model <- keras_model_sequential()

lstm_model %>%
  layer_lstm(units = 50, # size of the layer
       batch_input_shape = c(1, 12, 1), # batch size, timesteps, features
       return_sequences = TRUE,
       stateful = TRUE) %>%
  # fraction of the units to drop for the linear transformation of the inputs
  layer_dropout(rate = 0.5) %>%
  layer_lstm(units = 50,
        return_sequences = TRUE,
        stateful = TRUE) %>%
  layer_dropout(rate = 0.5) %>%
  time_distributed(keras::layer_dense(units = 1))
```

You can also decide to try out different `activation functions` with activation parameter (hyperbolic tangent `tanh is the default one`).

Also `choose loss function for the optimization`, type of optimizer and metric for assessing the model performance. Info about different optimizers can he found here.
```{r}
lstm_model %>%
    compile(loss = 'mae', optimizer = 'adam', metrics = 'accuracy')

summary(lstm_model)
```

Next, we can` fit our stateful LSTM`. We set `shuffle = FALSE to preserve sequences of time series`.
```{r}
lstm_model %>% fit(
    x = x_train_arr,
    y = y_train_arr,
    batch_size = 1,
    epochs = 20,
    verbose = 0,
    shuffle = FALSE
)
```

# Perform the prediction
```{r}
lstm_forecast <- lstm_model %>%
    predict(x_pred_arr, batch_size = 1) %>%
    .[, , 1]

# we need to rescale the data to restore the original values
lstm_forecast <- lstm_forecast * scale_factors[2] + scale_factors[1]
```

# Forecast object
As we have the values predicted, we can turn the results into the `forecast object`, as we would get if using the forecast package. That will allow i.e. to use the forecast::autoplot function to plot the results of the prediction. In order to do so, we need to define several objects that build a forecast object.

# Prediction on train set
```{r}

 fitted <- predict(lstm_model, x_train_arr, batch_size = 1) %>%
     .[, , 1]
```

Prediction on a training set will provide us with 12 results for each input period. So we need to transform the data to get only one prediction per each date.

```{r}
if (dim(fitted)[2] > 1) {
    fit <- c(fitted[, 1], fitted[dim(fitted)[1], 2:dim(fitted)[2]])
} else {
    fit <- fitted[, 1]
}
 
# additionally we need to rescale the data
fitted <- fit * scale_factors[2] + scale_factors[1]
nrow(fitted) # 562
```

Due to the fact that our forecast starts with 12 months offset, we need to provide artificial (or real) values for those months:

```{r}
# I specify first forecast values as not available
fitted <- c(rep(NA, lag), fitted)

```

# prediction in a form of ts object

```{r}
lstm_forecast <- timetk::tk_ts(lstm_forecast,
    start = c(2015, 5),
    end = c(2016, 4),
    frequency = 12)
```

# input series
Additionally we need to transform the economics data into a time series object.
```{r}
input_ts <- timetk::tk_ts(economics$unemploy, 
    start = c(1967, 7), 
    end = c(2015, 4), 
    frequency = 12)
```

#forecast object
Finally we can `define the forecast object`:

```{r}
forecast_list <- list(
    model = NULL,
    method = "LSTM",
    mean = lstm_forecast,
    x = input_ts,
    fitted = fitted,
    residuals = as.numeric(input_ts) - as.numeric(fitted)
  )

class(forecast_list) <- "forecast"
```

Now we can easily `plot the data`:

```{r}
forecast::autoplot(forecast_list)
```






























































